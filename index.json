[{"authors":["admin"],"categories":null,"content":"I am a statistician whose research involves methodological development, and is based largely on motivation from questions in the social sciences, social epidemiology and environmetrics.\nMy recent work is on statistical models for social networks, network inference, the development of statistical methodology for the collection and analysis of social network data, surveying of hard-to-reach populations, spatial processes and demography.\n","date":1655596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1655596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https:handcock.github.io/author/mark-s.-handcock/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mark-s.-handcock/","section":"authors","summary":"I am a statistician whose research involves methodological development, and is based largely on motivation from questions in the social sciences, social epidemiology and environmetrics.\nMy recent work is on statistical models for social networks, network inference, the development of statistical methodology for the collection and analysis of social network data, surveying of hard-to-reach populations, spatial processes and demography.","tags":null,"title":"Mark S. Handcock","type":"authors"},{"authors":null,"categories":null,"content":"This course is an introduction to computational statistics through numerical methods and computationally intensive methods for statistical problems. Topics include statistical graphics, root finding, simulation, randomization testing, and bootstrapping. Covers intermediate to advanced programming with R.\nMotivation and Synopsis During the twentieth century, the development of statistical computing played a crucial facilitating role for the growth of the statistics discipline and the adoption of statistical methods within the scientiﬁc community and beyond. In the twenty-ﬁrst century digital age, the amounts of data available for statistical analysis has grown tremendously, yielding new opportunities for statistical computing, as well as new challenges. Statistical computing constitutes an important part of a statistics education, and is highly valuable for statisticians in both academia and industry.\nThis course is designed to provide the upper-division statistics student with the fundamentals of statistical computing, particularly through use of the language R.\nThe course is thematically split into two parts. The ﬁst part will focus on learning the tools and the necessary skills to perform computational statistics. Students will learn intermediate to advanced R programming and usage of some of its functions and packages. The student will learn how to develop functions and packages for the management, pre-processing and analysis of statistical data. The second part of the class will focus on some foundational methods in computational statistics. This includes numerical methods such as root ﬁnding, numeric integration, and mathematical optimization. It will continue on to cover the generation of random variables, simulation, and Monte Carlo methods to answer statistical questions.\nThe computer is the scientific laboratory of the statistician. It plays the same role for the statistical research as the traditional laboratories play for physics and chemistry researchers. As such this course should allow the student to develop a degree of comfort and competence \"in the lab.”\nThe primary purpose of this course is to provide students with a common set of core knowledge about statistical computing computing for their class work and research. The course will have an applied focus on tools. The course will involve the practical application of the ideas of statistical computing and their implementation through statistical software, particularly R.\nSyllabus of the Course Lecture \u0026nbsp;Topics 1 Introduction, the R language and eco-system 2 Data structures and their management 3 R programming and writing functions 4 Importing data, web scraping, manipulating data with tidyr and dpyr 5 Visualization and graphics (ggplot2) 6 Numerical methods: floating point arithmetic, root finding 7 Numerical methods: basic optimization 8 Random numbers, random variables, and simulation in R 9 Randomization tests, permutation tests and bootstrapping 10 Additional topics: Monte Carlo-integration, kernel density estimation A detailed description of the class is available here.\n","date":1479945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479945600,"objectID":"1dcdb082ac2522a88b8b327a6d86e8e2","permalink":"https:handcock.github.io/teaching/102a/","publishdate":"2016-11-24T00:00:00Z","relpermalink":"/teaching/102a/","section":"teaching","summary":"This course is an introduction to computational statistics through numerical methods and computationally intensive methods for statistical problems. Topics include statistical graphics, root finding, simulation, randomization testing, and bootstrapping. Covers intermediate to advanced programming with R.","tags":["courses"],"title":"STATS 102A: Introduction to Computational Statistics with R","type":"teaching"},{"authors":null,"categories":null,"content":"This course considers the impacts that data collected today have upon individuals and society. Rapid increase in scale and types of data collected has impacted commerce and society in new ways. Consideration of economic, social and ethical, legal and political impacts of data, especially that collected on human behavior. Topics include privacy and data protection, intellectual property and confidentiality, sample selection and algorithms, equality and anti-discrimination.\nThe Bruin Learn course page is here.\nMotivation and Synopsis During the twentieth century, the development of statistical computing played a crucial facilitating role for the growth of the statistics discipline and the adoption of statistical methods. In the twenty-first century digital age, the amounts of data available for statistical analysis has grown tremendously, yielding new opportunities , as well as new challenges.\nThis course considers the impacts that the data collected today have upon individuals and society. The rapid increase in the scale and types of data collected has impacted commerce and society in new ways. In this course we consider the economic, social and ethical, legal and political impacts of data, especially that collected on human behavior. Particular topics will be privacy and data protection, intellectual property and confidentiality, sample selection and algorithms, equality and anti-discrimination.\nThis course is intended to provide students sociological, psychological and economic lenses to explore how our increasingly digital lifestyle changes institutions and social relations. These lenses are then used to guide data scientists in their work.\nThe course has various parts:\nWe\u0026rsquo;ll consider what data is and the growth of Big Data. We will consider the role data play in the broader information context and how that context influences the data we collect and the related statistical issues. We will consider statistical techniques to improve privacy and data protection, confidentiality, sample selection and algorithms, equality and anti-discrimination. Syllabus of the Course Lecture \u0026nbsp;Topics 1 Introduction: What, really, is Data? 2 Contextualizing Data 3 Small Data and Big Data 4 Datafication and its Implications 5 Statistical methods to preserve privacy 6 Sample selection and algorithms 7 Equality and anti-discrimination\n8 Intellectual and Personal Property 9 Additional topics ","date":1641168e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168e3,"objectID":"8c798203eccd3c08d0d609778cbefa37","permalink":"https:handcock.github.io/teaching/184/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/teaching/184/","section":"teaching","summary":"This course considers the impacts that data collected today have upon individuals and society. Rapid increase in scale and types of data collected has impacted commerce and society in new ways.","tags":["courses"],"title":"STATS 184: Societal Impacts of Data","type":"teaching"},{"authors":null,"categories":null,"content":"This course is a survey of computational methods that are especially useful for statistical analysis, with implementations in statistical package R. Topics include matrix analysis, multivariate regression, principal component analysis, multivariate analysis, and deterministic optimization methods.\nThe Bruin Learn course page is here.\nA detailed description of the class is available here.\nMotivation and Synopsis During the twentieth century, the development of statistical computing played a crucial facilitating role for the growth of the statistics discipline and the adoption of statistical methods within the scientiﬁc community and beyond. In the twenty-ﬁrst century digital age, the amounts of data available for statistical analysis has grown tremendously, yielding new opportunities for statistical computing, as well as new challenges. Statistical computing constitutes an important part of a statistics education, and is highly valuable for statisticians in both academia and industry.\nThis course is an introduction computational methods that are useful for statistical analysis, with implementations in the statistical package R.\nComputational methods are essential to both understand and implement modern statistical ideas. Often the computation methods are a translation of the statistical methods into another language, one that computer understand. In this class we will consider this process for key statistical models and techniques. These include multivariate regression, principal component analysis, and multivariate analysis. In doing so, we study the core mathematical and numerical ideas that make this possible including matrix analysis and deterministic optimization methods.\nThe course has various parts:\nWe\u0026rsquo;ll do some matrix algebra (not linear algebra), emphasizing what is available in R. We apply the matrix algebra to multivariate data analysis, in particular to regression, principal component analysis, canonical analysis, correspondence analysis. We discuss optimization methods. These will be mainly deterministic but also stochastic methods We finally apply both optimization and linear algebra to discuss statistical techniques such as multidimensional scaling, independent component analysis, tomography, and generalized mixed linear models. Well, maybe not all. The primary purpose of this course is to provide students with a common set of core knowledge about statistical computing computing for their class work and research. The course will have an applied focus on tools. The course will involve the practical application of the ideas of statistical computing and their implementation through statistical software, particularly R.\n","date":1650326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650326400,"objectID":"d3e2a7a48bf33e71880311b2406d69a4","permalink":"https:handcock.github.io/teaching/202b/","publishdate":"2022-04-19T00:00:00Z","relpermalink":"/teaching/202b/","section":"teaching","summary":"This course is a survey of computational methods that are especially useful for statistical analysis, with implementations in statistical package R. Topics include matrix analysis, multivariate regression, principal component analysis, multivariate analysis, and deterministic optimization methods.","tags":["courses"],"title":"STATS 202B: Matrix Algebra and Optimization","type":"teaching"},{"authors":null,"categories":null,"content":"This course is a survey of Monte Carlo methods and numerical integration. Importance and rejection sampling. Sequential importance sampling. Markov chain Monte Carlo (MCMC) sampling techniques, with emphasis on Gibbs samplers and Metropolis/Hastings. Simulated annealing. Exact sampling with coupling from past. Permutation testing and bootstrap confidence intervals.\nA detailed description of the class is available here.\nMotivation and Synopsis During the twentieth century, the development of statistical computing played a crucial facilitating role for the growth of the statistics discipline and the adoption of statistical methods within the scientiﬁc community and beyond. In the twenty-ﬁrst century digital age, the amounts of data available for statistical analysis has grown tremendously, yielding new opportunities for statistical computing, as well as new challenges. Statistical computing constitutes an important part of a statistics education, and is highly valuable for statisticians in both academia and industry.\nThis graduate level course introduces Monte Carlo methods for simulation, optimization, estimation, learning and complex landscape visualization, including: Importance sampling; Sequential importance sampling; Markov chain Monte Carlo (MCMC) sampling techniques including Gibbs samplers, Metropolis/Hastings and various improvements; Simulated annealing; Exact sampling techniques; Convergence analysis; Data augmentation; Cluster sampling, such as Swendsen-Wang and SW-cuts; Hamiltonian and Langevin Monte Carlo; Equi-energy and multi-domain sampler; and Techniques for mapping complex energy landscapes\nThe primary purpose of this course is to provide students with a common set of core knowledge about statistical computing computing for their class work and research. The course will have an applied focus on tools. The course will involve the practical application of the ideas of statistical computing and their implementation through statistical software, particularly R.\nPrerequisites\nStat 202B: Matrix Algebra and Optimization. People who didn’t take 202B before may still take this class as long as they have background on matrix algebra, probability theory, and programming skills. To do this attend the first classes and we can assess if this is advisable. ","date":1609632e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609632e3,"objectID":"f637e40bc86650f8c464d0e2bf95ad95","permalink":"https:handcock.github.io/teaching/202c/","publishdate":"2021-01-03T00:00:00Z","relpermalink":"/teaching/202c/","section":"teaching","summary":"This course is a survey of Monte Carlo methods and numerical integration. Importance and rejection sampling. Sequential importance sampling. Markov chain Monte Carlo (MCMC) sampling techniques, with emphasis on Gibbs samplers and Metropolis/Hastings.","tags":["courses"],"title":"STATS 202C: Monte Carlo Methods for Optimization","type":"teaching"},{"authors":null,"categories":null,"content":"This course is designed for social sciences graduate students and advanced undergraduate students seeking training in data issues and methods employed in social sciences.\nThe Bruin Learn course page is here.\nA detailed description of the class is available here.\nMotivation and Synopsis Statistics C116/C216 is a second course in social statistics and will focus on Bayesian statistical analysis. It will cover the principles of Bayesian statistics, Bayesian data analysis and modeling. In particular it will develop the ideas in the context of linear regression, its non-linear generalizations, and hierarchical models. While the idea and principles are general, all applications and models will be chosen from those most relevant to the social sciences.\nThis course is designed for graduate students majoring in Statistics, the Social Sciences and advanced undergraduate students who are planning to attend graduate school.\nThis course is most appropriate for student seeking additional training in the application of Bayesian statistics to data and are looking for an introductory course that advances rapidly. The ultimate goal is to equip students with the analytical and programming skills necessary to address social statistics problems within the Bayesian paradigm based on current data and technologies.\nThe course has various perspectives:\nIt focuses on conceptual understanding of the Bayesian statistics. It focuses on conceptual understanding of the the primary models used in social statistics. It involves the analysis of real-data. It involves implementing the methods using freely available software. The course will involve the practical application of the ideas of statistical computing and their implementation through statistical software, particularly R. As statistical computation is essential for many of the modeling approaches, expertise will need to be developed.\n","date":1633219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633219200,"objectID":"3b5439321bff995d19aac4394582e270","permalink":"https:handcock.github.io/teaching/c216/","publishdate":"2021-10-03T00:00:00Z","relpermalink":"/teaching/c216/","section":"teaching","summary":"This course is designed for social sciences graduate students and advanced undergraduate students seeking training in data issues and methods employed in social sciences.\nThe Bruin Learn course page is here.","tags":["courses"],"title":"STATS C216/C116: Social Statistics","type":"teaching"},{"authors":null,"categories":null,"content":"This course is a introduction to the analysis of social structure, conceived in terms of social relationships. Major concepts of social network theory and mathematical representation of social concepts such as role and position. Use of graphical representations of network information.\nA detailed description of the class is available here.\nMotivation and Synopsis This course is an introduction to the analysis of social structure, conceived in terms of social relationships.\nSocial structure is conceptualized as a system of social relations tieing distinct social entities to one another. Social network theory is the attempt to represent the structure in social relations via networks. It is then a theory pertaining to types of observable social spaces and their relation to individual and group behavior.\nObservations on the social relations are of two forms:\nindividual level information on the social entities relational data on pairs of entities While both forms are important for the study of social relations, social network theory recognizes fundamental role of the relational information. It is based on the premise that social context is an important determinant of individual behavior. It seeks to understand individual and group behavior in terms of relational information rather than as solely the aggregation of individual characteristics.\nThe focus of the course are modern methods for the statistical analysis of social networks. The course covers the major concepts of social network theory and the mathematical representation of social concepts such as \u0026ldquo;role\u0026rdquo; and \u0026ldquo;position\u0026rdquo;.\nVisualization plays a central in social network analysis. With the development of high speed computing and graphical display tools, visualization has become a flexible and powerful tool in the exploration of social relations. Graphics exploit the power of our visual senses to convey information in a direct way. In this course we will emphasize the use of graphical representations of network information as much as possible.\n","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"eb697444bd01bb6aff69d23f225b215c","permalink":"https:handcock.github.io/teaching/218/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/teaching/218/","section":"teaching","summary":"This course is a introduction to the analysis of social structure, conceived in terms of social relationships. Major concepts of social network theory and mathematical representation of social concepts such as role and position.","tags":["courses"],"title":"STATS 218: Statistical Analysis of Networks","type":"teaching"},{"authors":[],"categories":null,"content":"","date":1657756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657756800,"objectID":"e3ad8cac8438b03b071b6e99cf5adb87","permalink":"https:handcock.github.io/talk/sunbelt22-ensemble-lolog/","publishdate":"2021-12-12T17:11:21Z","relpermalink":"/talk/sunbelt22-ensemble-lolog/","section":"talk","summary":"We assess the real-world performance of Latent Order Logistic models (LOLOG) when applied to typical networks modelled by researchers by comparing them to Exponential-family random graph models (ERGMs). We demonstrate that the LOLOG models are, in general, in qualitative agreement with the ERGM models, and provide at least as good a model fit. In addition, they are typically faster and easier to fit to data, without the tendency for degeneracy that plagues ERGMs. This is joint work with Duncan A. Clark.","tags":["Degeneracy","ERGM","goodness of fit","LOLOG","social network analysis","social network modelling"],"title":"Comparing the real-world performance of exponential-family random graph models and latent order logistic models for social network analysis","type":"talk"},{"authors":["Bart Blackburn","Mark S. Handcock"],"categories":[],"content":"","date":1655596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655596800,"objectID":"365194bc789656eb4c0aad8bf9cef38a","permalink":"https:handcock.github.io/publication/tapering/","publishdate":"2022-06-19T00:00:00Z","relpermalink":"/publication/tapering/","section":"publication","summary":"Exponential-family Random Graph Models (ERGMs) have long been at the forefront of the analysis of relational data. The exponential-family form allows complex network dependencies to be represented. Models in this class are interpretable, flexible and have a strong theoretical foundation. The availability of powerful user-friendly open-source software allows broad accessibility and use. However, ERGMs sometimes suffer from a serious condition known as near-degeneracy, in which the model exhibits unrealistic probabilistic behavior or a severe lack-of-fit to real network data. Recently, Fellows and Handcock (2017) proposed a new model class, the Tapered ERGM, which circumvents the issue of near-degeneracy while maintaining the desirable features of ERGMs. However, the question of how to determine the proper amount of tapering needed for any model was heretofore left unanswered. This paper develops a new methodology for how to determine the necessary level of tapering and as such provides a new approach to inference for the Tapered ERGM class. Noting that a Tapered ERGM can always be made non-degenerate, we offer data-driven approaches for determining the amount of tapering necessary. The mean-value parameter estimates are unaffected by tapering, and we show that the natural parameter estimates are numerically weakly varying by the level of tapering. We then apply the Tapered ERGM to two published networks to demonstrate its effectiveness in cases where typical ERGMs fail and present the case for Tapered ERGMs replacing ERGMs entirely. Accepted.","tags":["Exponential-family Random Graph Model","Social Network Analysis","Degeneracy","Goodness-of-Fit"],"title":"Practical Network Modeling via Tapered Exponential-family Random Graph Models","type":"publication"},{"authors":[],"categories":null,"content":"","date":1654705800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654705800,"objectID":"c74dd85a51056e48622362cd86d63a4b","permalink":"https:handcock.github.io/talk/itacosm22/","publishdate":"2021-12-12T17:11:21Z","relpermalink":"/talk/itacosm22/","section":"talk","summary":"This is an invited talk on survey sampling given at the 7th Italian Conference on Survey Methodology.","tags":[],"title":"Sampling Hard-to-Reach Populations","type":"talk"},{"authors":[],"categories":null,"content":"","date":1653984e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653984e3,"objectID":"4828a5be1b1f900dc40631ab7c8358e0","permalink":"https:handcock.github.io/talk/ssc-icmic22/","publishdate":"2021-12-12T17:11:21Z","relpermalink":"/talk/ssc-icmic22/","section":"talk","summary":"This is the Presidential Invited Address, Survey Methods Section, Statistical Society of Canada Annual Meeting.","tags":[],"title":"Sampling Hard-to-Reach Populations","type":"talk"},{"authors":["Duan Mengjuan","Mark S. Handcock","Bart Blackburn","Fiona Kee","Viema Biaukula","Tamano Matsuic","Babatunde Olowokurec"],"categories":[],"content":"","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"1567c1dc9709c69d0c12bcec0de79a1a","permalink":"https:handcock.github.io/publication/who-acm-calc/","publishdate":"2022-05-24T21:28:45Z","relpermalink":"/publication/who-acm-calc/","section":"publication","summary":"A method was developed to track all-cause mortality (ACM) and an online open-source user-friendly interface was developed for its use.","tags":["All-Cause Mortality","Excess Deaths","Statistical Model"],"title":"Tool for tracking all-cause mortality and estimating excess mortality to support the COVID-19 pandemic response","type":"publication"},{"authors":[],"categories":null,"content":"","date":1653411600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653411600,"objectID":"1493b8a725a57bafdff651cfd5867096","permalink":"https:handcock.github.io/talk/intheworld2205/","publishdate":"2021-12-12T17:11:21Z","relpermalink":"/talk/intheworld2205/","section":"talk","summary":"This event provided an opportunity for students to hear from leading data scientists about how their world has been changed by data. They shared how a data scientist can be successful in any career area or industry. I was the moderator at this event.","tags":[],"title":"Data Theory in the World Seminar: How Data Drives Business Decisions","type":"talk"},{"authors":null,"categories":null,"content":"After many years with a hand written website in HTML, I moved to using the Hugo framework with the Academic theme. The motivation for this is the work of Qingyuan Zhao who deserved the credit for both creating the template and also publishing it so that others can benefit from it. Thank you, Qingyuan Zhao!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"ee42c466b30f21ae7250698f994671a9","permalink":"https:handcock.github.io/post/migrating/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/post/migrating/","section":"post","summary":"After many years with a hand written website in HTML, I moved to using the Hugo framework with the Academic theme. The motivation for this is the work of Qingyuan Zhao who deserved the credit for both creating the template and also publishing it so that others can benefit from it.","tags":["Workflow"],"title":"Migrating my website to Hugo + Academic","type":"post"},{"authors":null,"categories":null,"content":"I am an affiliate of the California Center for Population Research at UCLA.\n","date":1650326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650326400,"objectID":"d57825f2e9246f4e0c9de3b14a0384e3","permalink":"https:handcock.github.io/research/ccpr/","publishdate":"2022-04-19T00:00:00Z","relpermalink":"/research/ccpr/","section":"research","summary":"I am an affiliate of the California Center for Population Research at UCLA.","tags":["courses"],"title":"CCPR","type":"research"},{"authors":["Marilyn N. Raphael","Mark S. Handcock"],"categories":[],"content":"","date":1646870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646870400,"objectID":"58d88fb4430d0f8c931e8b38f9b87667","permalink":"https:handcock.github.io/publication/nature-min/","publishdate":"2022-03-10T21:28:45Z","relpermalink":"/publication/nature-min/","section":"publication","summary":"Antarctic sea ice extent reached a new record low of 1.965 million km2 on 23 February 2022. This extent is approximately 32% below climatological values and might indicate a transition to new, more extreme, annual fluctuations.","tags":["Climate Change","Environmetrics","Cryospheric science"],"title":"A New Record Minimum for Antarctic Sea Ice","type":"publication"},{"authors":["Duncan A. Clark","Mark S. Handcock"],"categories":[],"content":"","date":1643328e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643328e3,"objectID":"8e3c60436a8e35f7e6c8738713d07b4d","permalink":"https:handcock.github.io/publication/ensemble-lolog/","publishdate":"2022-01-28T00:00:00Z","relpermalink":"/publication/ensemble-lolog/","section":"publication","summary":"We assess the real-world performance of Latent Order Logistic models (LOLOG) when applied to typical networks modelled by researchers by comparing them to Exponential-family random graph models (ERGMs).  We demonstrate that the LOLOG models are, in general, in qualitative agreement with the ERGM models, and provide at least as good a model fit.  In addition, they are typically faster and easier to fit to data, without the tendency for degeneracy that plagues ERGMs.","tags":["Degeneracy","ERGM","goodness of fit","LOLOG","social network analysis","social network modelling"],"title":"Comparing the real-world performance of exponential-family random graph models and latent order logistic models for social network analysis","type":"publication"},{"authors":["Ryan L Fogt","Amanda M Sleinkofer","Marilyn N. Raphael","Mark S. Handcock"],"categories":[],"content":"","date":1641772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641772800,"objectID":"c9afea3f0d03b4c7a82d2ad44a1e84ea","permalink":"https:handcock.github.io/publication/nature-regime-shift/","publishdate":"2022-01-10T00:00:00Z","relpermalink":"/publication/nature-regime-shift/","section":"publication","summary":"In stark contrast to the Arctic, there have been statistically significant positive trends in total Antarctic sea ice extent since 1979. However, the short and highly variable nature of observed Antarctic sea ice extent limits the ability to fully understand the historical context of these recent changes. To meet this challenge, we have created robust, observation-based reconstruction ensembles of seasonal Antarctic sea ice extent since 1905. Using these reconstructions, here we show that the observed period since 1979 is the only time all four seasons demonstrate significant increases in total Antarctic sea ice in the context of the twentieth century and that the observed increases are juxtaposed against statistically significant decreases throughout much of the early and middle twentieth century. These reconstructions provide reliable estimates of seasonally resolved total Antarctic sea ice extent and are skilful enough to better understand aspects of air–sea–ice interactions within the Antarctic climate system.","tags":["Climate Change","Environmetrics","Cryospheric science"],"title":"A Regime Shift in Seasonal Total Antarctic Sea Ice Extent in the Twentieth Century","type":"publication"},{"authors":[],"categories":null,"content":"","date":1639065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639065600,"objectID":"88d3682faa96cf413f9e527a755e5558","permalink":"https:handcock.github.io/talk/whotag22/","publishdate":"2021-12-12T17:11:21Z","relpermalink":"/talk/whotag22/","section":"talk","summary":"","tags":[],"title":"Tracking all cause of death and estimating excess mortality during the COVID-19 pandemic: statistical and computational tools","type":"talk"},{"authors":["Alex D. Fraser","Robert A. Massom","Mark S. Handcock"," P. Reid"," K.  I. Ohshima"," M. N. Raphael"," J. Cartwright"," A. R. Klekociuk"," Z. Wang","R. Porter-Smith"],"categories":[],"content":"","date":1635897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635897600,"objectID":"24f89b18379ddc1e4bbcaa7fabafe1d1","permalink":"https:handcock.github.io/publication/fast-ice/","publishdate":"2021-11-03T00:00:00Z","relpermalink":"/publication/fast-ice/","section":"publication","summary":"Landfast sea ice (fast ice) is an important though poorly understood component of the cryosphere on the Antarctic continental shelf, where it plays a key role in atmosphere–ocean–ice-sheet interaction and coupled ecological and biogeochemical processes. Here, we present a first in-depth baseline analysis of variability and change in circum-Antarctic fast-ice distribution (including its relationship to bathymetry), based on a new high-resolution satellite-derived time series for the period 2000 to 2018. This reveals (a) an overall trend of −882±824 km2 yr−1 (−0.19±0.18 % yr−1) and (b) eight distinct regions in terms of fast-ice coverage and modes of formation. Of these, four exhibit positive trends over the 18-year period and four negative. Positive trends are seen in East Antarctica and in the Bellingshausen Sea, with this region claiming the largest positive trend of +1198±359 km2 yr−1 (+1.10±0.35 % yr−1). The four negative trends predominantly occur in West Antarctica, with the largest negative trend of −1206±277 km2 yr−1 (−1.78±0.41 % yr−1) occurring in the Victoria and Oates Land region in the western Ross Sea. All trends are significant. This new baseline analysis represents a significant advance in our knowledge of the current state of both the global cryosphere and the complex Antarctic coastal system, which are vulnerable to climate variability and change. It will also inform a wide range of other studies.","tags":["Climate Change","Environmetrics","Cryospheric science"],"title":"Eighteen-year record of circum-Antarctic landfast-sea-ice distribution allows detailed baseline characterisation and reveals trends and variability","type":"publication"},{"authors":["Mark S. Handcock","Andrew Hicks","Narayan Sastry","Anne R. Pebley"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"f9032e19c44e07a2340b9b21942bbfcf","permalink":"https:handcock.github.io/publication/nghbd-note/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/nghbd-note/","section":"publication","summary":"We revisit a novel causal model published in Demography by Hicks et al. (2018), designed to assess whether exposure to neighborhood disadvantage over time affects children's reading and math skills. Here, we provide corrected and new results. Reconsideration of the model in the original article raised concerns about bias due to exposure-induced confounding (i.e., past exposures directly affecting future exposures) and true state dependence (i.e., past exposures affecting confounders of future exposures). Through simulation, we show that our originally proposed propensity function approach displays modest bias due to exposure-induced confounding but no bias from true state dependence. We suggest a correction based on residualized values and show that this new approach corrects for the observed bias. We contrast this revised method with other causal modeling approaches using simulation. Finally, we reproduce the substantive models from Hicks et al. (2018) using the new residuals-based adjustment procedure. With the correction, our findings are essentially identical to those reported originally. We end with some conclusions regarding approaches to causal modeling.","tags":["Child Development","Neighborhoods","Residential histories","Propensity function models"],"title":"A Note on 'Sequential Neighborhood Effects' by Hicks et al. (2018)","type":"publication"},{"authors":["Marilyn N. Raphael","Mark S. Handcock","Marika M Holland","Laura L Landrum"],"categories":[],"content":"","date":1602892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602892800,"objectID":"2959f249dd63b676e049b7e9cbae56a0","permalink":"https:handcock.github.io/publication/assess-cesm2/","publishdate":"2020-10-17T00:00:00Z","relpermalink":"/publication/assess-cesm2/","section":"publication","summary":"Understanding the variability of Antarctic sea ice is an ongoing challenge given the limitations of observed data. Coupled climate model simulations present the opportunity to examine this variability in Antarctic sea ice. Here, the daily sea ice extent simulated by the newly released National Center for Atmospheric Research (NCAR) Community Eart h System Model Version 2 (CESM2) for the historical period (1979–2014) is compared to the satellite-observed daily sea ice extent for the same period. The comparisons are made using a newly developed suite of statistical metrics that estimates the variability of the sea ice extent on timescales ranging from the long-term decadal to the short term, intraday scales. Assessed are the annual cycle, trend, day-to-day change, and the volatility, a new statistic that estimates the variability at the daily scale. Results show that the trend in observed daily sea ice is dominated by subdecadal variability with a weak positive linear trend superimposed. The CESM2 simulates comparable subdecadal variability but with a strong negative linear trend superimposed. The CESM2's annual cycle is similar in amplitude to the observed, key differences being the timing of ice advance and retreat. The sea ice begins its advance later, reaches its maximum later and begins retreat later in the CESM2. This is confirmed by the day-to-day change. Apparent in all of the sea ice regions, this behavior suggests the influence of the semiannual oscillation of the circumpolar trough. The volatility, which is associated with smaller scale dynamics such as storms, is smaller in the CESM2 than observed.","tags":["Climate Change","Environmetrics","Cryospheric science"],"title":"An Assessment of the Temporal Variability in the Annual Cycle of Daily Antarctic Sea Ice in the NCAR Community Earth System Model, Version 2: A Comparison of the Historical Runs with Observations","type":"publication"},{"authors":null,"categories":null,"content":"","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"7dc79766d9f3e810b5b5d2606ed32475","permalink":"https:handcock.github.io/project/covid-19/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/project/covid-19/","section":"project","summary":"Project page for some work on the COVID-19 pandemic.","tags":["Infectious-Diseases"],"title":"COVID-19","type":"project"},{"authors":["Mark S. Handcock","Marilyn N. Raphael"],"categories":[],"content":"","date":1593648e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593648e3,"objectID":"d3ab37bc5834201ea55926c1ee363059","permalink":"https:handcock.github.io/publication/annual-cycle/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/publication/annual-cycle/","section":"publication","summary":"The total Antarctic sea ice extent (SIE) experiences a distinct annual cycle, peaking in September and reaching its minimum in February. In this paper we propose a mathematical and statistical decomposition of this temporal variation in SIE. Each component is interpretable and, when combined, gives a complete picture of the variation in the sea ice. We consider timescales varying from the instantaneous and not previously defined to the multi-decadal curvilinear trend, the longest. Because our representation is daily, these timescales of variability give precise information about the timing and rates of advance and retreat of the ice and may be used to diagnose physical contributors to variability in the sea ice. We define a number of annual cycles each capturing different components of variation, especially the yearly amplitude and phase that are major contributors to SIE variation. Using daily sea ice concentration data, we show that our proposed invariant annual cycle explains 29 % more of the variation in daily SIE than the traditional method. The proposed annual cycle that incorporates amplitude and phase variation explains 77 % more variation than the traditional method. The variation in phase explains more of the variability in SIE than the amplitude. Using our methodology, we show that the anomalous decay of sea ice in 2016 was associated largely with a change of phase rather than amplitude. We show that the long term trend in Antarctic sea ice extent is strongly curvilinear and the reported positive linear trend is small and dependent strongly on a positive trend that began around 2011 and continued until 2016.","tags":["Climate Change","Environmetrics","Cryospheric science"],"title":"Modeling the annual cycle of daily Antarctic sea ice extent","type":"publication"},{"authors":["Medha Uppala","Mark S. Handcock"],"categories":[],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"45bebd2e58ea27f60ee48fd3baff0c92","permalink":"https:handcock.github.io/publication/wildfire/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/wildfire/","section":"publication","summary":"This paper focuses on spatial and temporal modeling of point processes on linear networks. Point processes on linear networks can simply be defined as point events occurring on or near line segment network structures embedded in a certain space. A separable modeling framework is introduced that posits separate formation and dissolution models of point processes on linear networks over time. While the model was inspired by spider web building activity in brick mortar lines, the focus is on modeling wildfire ignition origins near road networks over a span of 14 years. As most wildfires in California have human-related origins, modeling the origin locations with respect to the road network provides insight into how human, vehicular and structural densities affect ignition occurrence. Model results show that roads that traverse different types of regions such as residential, interface and wildland regions have higher ignition intensities compared to roads that only exist in each of the mentioned region types.","tags":["Point processes","linear network","spatiotemporal modeling","pseudolikelihood","Berman–Turner method","spider webs","wildfires","ignition origins","road networks"],"title":"Modeling wildfire ignition origins in southern California using linear network point processes","type":"publication"},{"authors":["Brian J Kim","Mark S. Handcock"],"categories":[],"content":"","date":1575676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575676800,"objectID":"3cd0830bf2d6cc52eaa551627e4b995e","permalink":"https:handcock.github.io/publication/pse2/","publishdate":"2019-12-07T00:00:00Z","relpermalink":"/publication/pse2/","section":"publication","summary":"Respondent-driven sampling (RDS) is commonly used to study hard-to-reach populations since traditional methods are unable to efficiently survey members due to the typically highly stigmatized nature of the population. The number of people in these populations is of primary global health and demographic interest and is usually hard to estimate. However, due to the nature of RDS, current methods of population size estimation are insufficient. We introduce a new method of estimating population size that uses concepts from capture-recapture methods while modeling RDS as a successive sampling process. We assess its statistical validity using information from the CDC’s National HIV Behavioral Surveillance system in 2009 and 2012.","tags":["Hard-to-reach population sampling","Network sampling","Model-based survey sampling","Capture-recapture","Without replacement sampling"],"title":"Population Size Estimation Using Multiple Respondent-Driven Sampling Surveys","type":"publication"},{"authors":["Alec M. Chan‐Golston","Sudipto Banerjee","Mark S. Handcock"],"categories":[],"content":"","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"315f75c11ab95171f85c8d25aba3a682","permalink":"https:handcock.github.io/publication/bayes-finite-pop/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/publication/bayes-finite-pop/","section":"publication","summary":"We develop a Bayesian model–based approach to finite population estimation accounting for spatial dependence. Our innovation here is a framework that achieves inference for finite population quantities in spatial process settings. A key distinction from the small area estimation setting is that we analyze finite populations referenced by their geographic coordinates. Specifically, we consider a two-stage sampling design in which the primary units are geographic regions, the secondary units are point-referenced locations, and the measured values are assumed to be a partial realization of a spatial process. Estimation of finite population quantities from geostatistical models does not account for sampling designs, which can impair inferential performance, whereas design-based estimates ignore the spatial dependence in the finite population. We demonstrate by using simulation experiments that process-based finite population sampling models improve model fit and inference over models that fail to account for spatial correlation. Furthermore, the process-based models offer richer inference with spatially interpolated maps over the entire region. We reinforce these improvements and demonstrate scalable inference for groundwater nitrate levels in the population of California Central Valley wells by offering estimates of mean nitrate levels and their spatially interpolated maps.","tags":["Bayesian modeling","finite population inference","hierarchical models","spatial process","two-stagesampling"],"title":"Bayesian inference for finite populations under spatial process settings","type":"publication"},{"authors":["Maryclare Griffin","Elena Erosheva","Krista J. Gile","Mark S. Handcock","Karen Fredriksen-Goldsen"],"categories":[],"content":"","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"04c35cf57f0b2334324a4508e5b2da42","permalink":"https:handcock.github.io/publication/griffin/","publishdate":"2018-11-13T00:00:00Z","relpermalink":"/publication/griffin/","section":"publication","summary":"Respondent-driven sampling (RDS) is a method for sampling from a target population by leveraging social connections.  RDS is invaluable to the study of hard-to-reach populations.  However, RDS is costly and can be infeasible.  RDS is infeasible when RDS point estimators have small effective sample sizes (large design effects) or when RDS interval estimators have large confidence intervals relative to estimates obtained in previous studies or poor coverage.  As a result, researchers need tools to assess whether or not estimation of certain characteristics of interest for specific populations is feasible in advance.  In this paper, we develop a simulation-based framework for using pilot data—in the form of a convenience sample of aggregated, egocentric data and estimates of subpopulation sizes within the target population—to assess whether or not RDS is feasible for estimating characteristics of a target population.  In doing so, we assume that more is known about egos than alters in the pilot data, which is often the case with aggregated, egocentric data in practice.  We build on existing methods for estimating the structure of social networks from aggregated, egocentric sample data and estimates of subpopulation sizes within the target population.  We apply this framework to assess the feasibility of estimating the proportion male, proportion bisexual, proportion depressed and proportion infected with HIV/AIDS within three spatially distinct target populations of older lesbian, gay and bisexual adults using pilot data from the Caring and Aging with Pride Study and the Gallup Daily Tracking Survey. We conclude that using an RDS sample of 300 subjects is infeasible for estimating the proportion male, but feasible for estimating the proportion bisexual, proportion depressed and proportion infected with HIV/AIDS in all three target populations.","tags":["aggregated egocentric sample data","hard to reach populations","network sampling","Respondent-driven sampling","social networks"],"title":"A Simulation-Based Framework for Assessing the Feasibility of Respondent-Driven Sampling for Estimating Characteristics in Populations of Lesbian, Gay and Bisexual Older Adults","type":"publication"},{"authors":["Sanjay Chaudhuri","Mark S. Handcock"],"categories":[],"content":"","date":1520294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520294400,"objectID":"af3a25597e23908a453d4bfd28a86984","permalink":"https:handcock.github.io/publication/emplik/","publishdate":"2018-03-06T00:00:00Z","relpermalink":"/publication/emplik/","section":"publication","summary":"We consider an empirical likelihood framework for inference for a statistical model based on an informative sampling design. Covariate information is incorporated both through the weights and the estimating equations. The estimator is based on conditional weights. We show that under usual conditions, with population size increasing unbounded, the estimates are strongly consistent, asymptotically unbiased and normally distributed. Our framework provides additional justification for inverse probability weighted score estimators in terms of conditional empirical likelihood. In doing so, it bridges the gap between design-based and model-based modes of inference in survey sampling settings. We illustrate these ideas with an application to an electoral survey.","tags":["Complex survey data","Design weights","Model parameter estimation","Conditional likelihood","Inverse probability weighted estimation","Design-based survey inference","Generalised linear models"],"title":"A Conditional Empirical Likelihood Based Method for Model Parameter Estimation from Complex survey Datasets","type":"publication"},{"authors":["Krista J. Gile","Isabelle S. Beaudry","Mark S. Handcock","Miles Q. Ott"],"categories":[],"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"9371b1fa1b22e43bd685039c8dd11c01","permalink":"https:handcock.github.io/publication/annrevrds/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/publication/annrevrds/","section":"publication","summary":"Respondent-driven sampling is a commonly used method for sampling from hard-to-reach human populations connected by an underlying social network of relations.  Beginning with a convenience sample, participants pass coupons to invite their contacts to join the sample.  Although the method is often effective at attaining large and varied samples, its reliance on convenience samples, social network contacts, and participant decisions makes it subject to a large number of statistical concerns.  This article reviews inferential methods available for data collected by respondent-driven sampling.","tags":["respondent-driven sampling","link-tracing","not missing at random","network sampling","social network","hard-to-reach population","snowball sampling"],"title":"Methods for Inference from Respondent-Driven Sampling Data","type":"publication"},{"authors":["Andrew Hicks","Mark S. Handcock","Narayan Sastry","Anne R. Pebley"],"categories":[],"content":"","date":1512e6,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512e6,"objectID":"d221e411f68d80bcf55319caf93bf4e5","permalink":"https:handcock.github.io/publication/nghbd/","publishdate":"2017-11-30T00:00:00Z","relpermalink":"/publication/nghbd/","section":"publication","summary":"Prior research has suggested that children living in a disadvantaged neighborhood have lower achievement test scores, but these studies typically have not estimated causal effects that account for neighborhood choice. Recent studies used propensity score methods to account for the endogeneity of neighborhood exposures, comparing disadvantaged and nondisadvantaged neighborhoods. We develop an alternative propensity function approach in which cumulative neighborhood effects are modeled as a continuous treatment variable. This approach offers several advantages. We use our approach to examine the cumulative effects of neighborhood disadvantage on reading and math test scores in Los Angeles. Our substantive results indicate that recency of exposure to disadvantaged neighborhoods may be more important than average exposure for children's test scores. We conclude that studies of child development should consider both average cumulative neighborhood exposure and the timing of this exposure.","tags":["Child Development","Neighborhoods","Residential histories","Propensity function models"],"title":"Sequential Neighborhood Effects: The Effect of Long-Term Exposure to Concentrated Disadvantage on Children's Reading and Mathematical Skills","type":"publication"},{"authors":["Michael W. Spiller","Krista J. Gile","Mark S. Handcock","Corinne M. Mar","Cyprian Wejnert"],"categories":[],"content":"","date":1502928e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502928e3,"objectID":"ea4c5715aee3cffed3f9eda4e4564987","permalink":"https:handcock.github.io/publication/evalvarrds/","publishdate":"2017-08-17T00:00:00Z","relpermalink":"/publication/evalvarrds/","section":"publication","summary":"Respondent-Driven Sampling (RDS) is a network-based method for sampling hard-to-reach populations that is widely used by public health agencies and researchers worldwide. Estimation of population characteristics from RDS data is challenging due to the unobserved population network, and multiple point and variance estimators have been proposed. Research evaluating these estimators has been limited and largely focused on point estimation; this analysis is the first evaluation of multiple variance estimators currently in use. We evaluated the performance of RDS variance estimators via simulations of RDS on synthetic networked populations constructed from 40 RDS surveys of injection drug users in the United States. In these simulations, average design effects (DEs) were lower and average 95% confidence interval (CI) coverage percentages were higher than suggested in previous work: typical DE range = 1–3; average 95% CI coverage = 93%. However, DE and CI coverage vary across the 40 sets of simulations, suggesting that the characteristics of a given study should be evaluated to assess estimator performance. We also found that simulation results are sensitive to whether sampling is conducted with replacement and the approach used to create CIs. We conclude that CI coverage rates and DEs are often acceptable but not perfect and that RDS estimates are usually reliable in scenarios where RDS assumptions are met. While RDS estimation performed reasonably well, we found strong evidence that the simple random sample variance estimator and corresponding CIs significantly underestimate variance and should not be used to analyze RDS data.","tags":["respondent-driven sampling","link-tracing","not missing at random","network sampling","social network","hard-to-reach population","snowball sampling"],"title":"Evaluating Variance Estimators for Respondent‐Driven Sampling","type":"publication"},{"authors":["Ian E. Fellows","Mark S. Handcock"],"categories":[],"content":"","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"bb173f030b817245f40d3f7e831c8b7c","permalink":"https:handcock.github.io/publication/phasetapering/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/publication/phasetapering/","section":"publication","summary":"Gibbs measures are a fundamental class of distributions for the analysis of high dimensional data. Phase transitions, which are also known as degeneracy in the network science literature, are an emergent property of these models that well describe many physical systems.  However, the reach of the Gibbs measure is now far outside the realm of physical systems, and in many of these domains multiphase behavior is a nuisance. This nuisance often makes distribution fitting impossible due to failure of the MCMC sampler, and even when an MLE fit is possible, if the solution is near a phase transition point, the plausibility of the fit can be highly questionable. We introduce a modification to the Gibbs distribution that reduces the effects of phase transitions, and with properly chosen hyper-parameters, provably removes all multiphase behavior. We show that this new distribution is just as easy to fit via MCMCMLE as the Gibbs measure, and provide examples in the Ising model from statistical physics and ERGMs from network science.","tags":["respondent-driven sampling","link-tracing","not missing at random","network sampling","social network","hard-to-reach population","snowball sampling"],"title":"Removing Phase Transitions from Gibbs Measures","type":"publication"},{"authors":["Krista J. Gile","Mark S. Handcock"],"categories":[],"content":"","date":1475107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475107200,"objectID":"ced95521a4c9f52fd25d539e843a7eea","permalink":"https:handcock.github.io/publication/netmiss/","publishdate":"2016-09-29T00:00:00Z","relpermalink":"/publication/netmiss/","section":"publication","summary":"It is common in the analysis of social network data to assume a census of the networked population of interest.  Often the observations are subject to partial observation due to a known sampling or unknown missing data mechanism.  However, most social network analysis ignores the problem of missing data by including only actors with complete observations.  We address the modelling of networks with missing data, developing previous ideas in missing data, network modelling and network sampling.  We use several methods including the mean value parameterization to show the quantitative and substantive differences between naive and principled modelling approaches.  We also develop goodness-of-fit techniques to understand model fit better.  The ideas are motivated by an analysis of a friendship network from the National Longitudinal Study of Adolescent Health.","tags":["AddHealth","exponential random graph model","maximum likelihood estimation","nonresponse","sample survey","statnet"],"title":"Analysis of Networks with Missing Data with Application to the National Longitudinal Study of Adolescent Health","type":"publication"},{"authors":["Mark S. Handcock","Krista J. Gile"],"categories":[],"content":"","date":1318982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1318982400,"objectID":"141e37453b969dcd6903bfb0554feb87","permalink":"https:handcock.github.io/publication/onsnowball/","publishdate":"2011-10-19T00:00:00Z","relpermalink":"/publication/onsnowball/","section":"publication","summary":"A phenomenon in the sociology of science is that multidisciplinary fields tend to produce a plethora of inconsistent terminology.  Often the meaning of a term evolves over time, or different terms are used for the same concept.  More confusing is the use of the same term for different concepts. The term 'snowball sampling' suffers from this treatment.  It has likely been in informal use for a long time, but it certainly predates Coleman (1958) and Trow (1957).  The earliest systematic work dates to the 1940s from the Columbia Bureau of Applied Social Research, led by Paul Lazarsfeld.  In this note we review the history of the term 'snowball sampling'.","tags":["respondent-driven sampling","link-tracing","not missing at random","network sampling","social network","hard-to-reach population","snowball sampling"],"title":"On the Concept of Snowball Sampling","type":"publication"},{"authors":["Michael S. Rendall","Mark S. Handcock","Stefan H. Jonsson"],"categories":[],"content":"","date":1233446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1233446400,"objectID":"dd1e478165ec7b72cae1479bd8a50068","permalink":"https:handcock.github.io/publication/bayesdemo/","publishdate":"2009-02-01T00:00:00Z","relpermalink":"/publication/bayesdemo/","section":"publication","summary":"Previous studies have demonstrated both large gains in efficiency and reductions in bias by incorporating population information in regression estimation with sample survey data. These studies, however, assumed that the population values are exact. This assumption is relaxed here through a Bayesian extension of constrained maximum likelihood estimation applied to U.S. Hispanic fertility. The Bayesian approach allows for the use of both auxiliary survey data and expert judgment in making adjustments to published Hispanic Population fertility rates, and for the estimation of uncertainty about these adjustments. Compared with estimation from sample survey data only, the Bayesian constrained estimator results in much greater precision in the age pattern of the baseline fertility hazard and therefore of the predicted values for any given combination of socioeconomic variables. The use of population data in combination with survey data may therefore be highly advantageous even when the population data are known to have significant levels of nonsampling error.","tags":["Hispanic Woman","Bayesian Estimation","Mean Square Deviation","Constraint Model","Unconstrained Model"],"title":"Bayesian estimation of hispanic fertility hazards from survey and population data","type":"publication"},{"authors":["Krista J. Gile","Mark S. Handcock"],"categories":[],"content":"","date":1157587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1157587200,"objectID":"dc9d18bed203660cf5500e600006e39a","permalink":"https:handcock.github.io/publication/cssswp_66/","publishdate":"2006-09-07T00:00:00Z","relpermalink":"/publication/cssswp_66/","section":"publication","summary":"Most inference using social network models assumes that the presence or absence of all relations is known. This is rarely the case. Most social network analysis ignores the problem of missing data by including only actors with complete observations. In this paper we use a statistical model for the underlying social network to demonstrate that the computationally parsimonious complete case approach can lead to different conclusions from an approach utilizing all observations. We also show that the overall fit to the data is improved by extending the model to represent differences between respondents and non-respondents. The ideas are motivated and illustrated by an analysis of a friendship network from the National Longitudinal Study of Adolescent Health.","tags":["AddHealth","exponential random graph model","maximum likelihood estimation","nonresponse","sample survey","statnet"],"title":"Model-based Assessment of the Impact of Missing Data on Inference for Networks","type":"publication"},{"authors":["Adam Glynn","Jon Wakefield","Mark S. Handcock","Thomas Richardson"],"categories":[],"content":"","date":1125964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1125964800,"objectID":"23475bb3c21982c18828e5b850d36a24","permalink":"https:handcock.github.io/publication/cssswp_51/","publishdate":"2005-09-06T00:00:00Z","relpermalink":"/publication/cssswp_51/","section":"publication","summary":"In this paper, we illustrate that combining ecological data with subsample data in situations in which a linear model is appropriate provides three main benefits.","tags":["ecological bias","combining information","within-area confounding","returns to education","sample design"],"title":"Alleviating Linear Ecological Bias and Optimal Design with Subsample Data","type":"publication"},{"authors":["Sanjay Chaudhuri","Mark S. Handcock","Michael S. Rendall"],"categories":[],"content":"","date":1115251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1115251200,"objectID":"472a2edde0208ec3978372c2f81b9d8b","permalink":"https:handcock.github.io/publication/cssswp_48/","publishdate":"2005-05-05T00:00:00Z","relpermalink":"/publication/cssswp_48/","section":"publication","summary":"In many situations information from a sample of individuals can be supplemented by population level information on the relationship between a dependent and explanatory variables. Inclusion of the population level information can reduce bias and increase the efficiency of the parameter estimates. Population level information can be incorporated via constraints on the model parameters. In general the constraints are nonlinear making the task of maximum likelihood estimation harder. In this paper we develop an alternative approach exploiting the notion of an empirical likelihood. It is shown that within the framework of generalised linear models, the population level information corresponds to linear constraints, which are comparatively easy to handle. We provide a two-step algorithm that produces parameter estimates using only unconstrained estimation. We also provide computable expressions for the standard errors. We give an application to demographic hazard modeling by combining panel survey data from the British Household Panel Survey (BHPS) with birth registration data.","tags":["Empirical Likelihood","Constrained Optimisation","Generalised Linear Models"],"title":"Generalised Linear Models Incorporating Population Level Information: An Empirical Likelihood Based Approach","type":"publication"},{"authors":["Mark S. Handcock","Adrian E. Raftery","Jeremy M. Tantrum"],"categories":[],"content":"","date":111456e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":111456e4,"objectID":"4398483472608d6a5ab6fcf3ccf4f673","permalink":"https:handcock.github.io/publication/cssswp_46/","publishdate":"2005-04-27T00:00:00Z","relpermalink":"/publication/cssswp_46/","section":"publication","summary":"Network models are widely used to represent relations among interacting units or actors. Network data often exhibit transitivity, meaning that two actors that have ties to a third actor are more likely to be tied than actors that do not, homophily by attributes of the actors or dyads, and clustering. Interest often focuses on  nding clusters of actors or ties, and the number of groups in the data is typically unknown. We propose a new model, the Latent Position Cluster Model (LPCM), under which the probability of a tie between two actors depends on the distance between them in an unobserved Euclidean social space, and the actors' locations in the latent social space arise from a mixture of distributions, each one corresponding to a cluster. We propose two estimation methods: a two-stage maximum likelihood method, and a Bayesian MCMC method; the former is quicker and simpler, but the latter performs better. We also propose a Bayesian way of determining the number of clusters present using approximate conditional Bayes factors. It models transitivity, homophily by attributes and clustering simultaneously, and does not require the number of clusters to be known. The model makes it easy to simulate realistic networks with clustering, potentially useful as inputs to models of more complex systems of which the network is part, such as epidemic models of infectious disease. We apply the model to two networks of social relations.","tags":["social networks","latent position models","methodology"],"title":"Model-Based Clustering for Social Networks","type":"publication"},{"authors":["David R. Hunter","Mark S. Handcock"],"categories":[],"content":"","date":1091491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1091491200,"objectID":"934137f308f6760e34ec1c44158ac0a8","permalink":"https:handcock.github.io/publication/cssswp_43/","publishdate":"2004-08-03T00:00:00Z","relpermalink":"/publication/cssswp_43/","section":"publication","summary":"Network data arise in a wide variety of applications. Although descriptive statistics for networks abound in the literature, the science of fitting statistical models to complex network data is still in its infancy. The models considered in this article are based on exponential families; therefore, we refer to them as exponential random graph models (ERGMs). Although ERGMs are easy to postulate, maximum likelihood estimation of parameters in these models is very difficult. In this article, we first review the method of maximum likelihood estimation using Markov chain Monte Carlo in the context of fitting linear ERGMs. We then extend this methodology to the situation where the model comes from a curved exponential family. The curved exponential family methodology is applied to new specifications of ERGMs, proposed by Snijders et al. (2004), having non-linear parameters to represent structural properties of networks such as transitivity and heterogeneity of degrees. We review the difficult topic of implementing likelihood ratio tests for these models, then apply all these model-fitting and testing techniques to the estimation of linear and non-linear parameters for a collaboration network between partners in a New England law firm.","tags":["exponential random graph model","maximum likelihood estimation","Markov chain Monte Carlo","p−star model"],"title":"Inference in curved exponential family models for networks","type":"publication"},{"authors":["David R. Hunter","Steven M. Goodreau","Mark S. Handcock"],"categories":[],"content":"","date":1083110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1083110400,"objectID":"7e07ec24ac5d69d594a7bb387c28c1f0","permalink":"https:handcock.github.io/publication/cssswp_47/","publishdate":"2004-04-28T00:00:00Z","relpermalink":"/publication/cssswp_47/","section":"publication","summary":"We present a systematic examination of real network datasets using maximum likelihood estimation for exponential random graph models as well as new procedures to evaluate how well the models fit the observed graphs. These procedures compare structural statistics of the observed graph with the corresponding statistics on graphs simulated from the fitted model. We apply this approach to the study of friendship relations among high school students from the National Longitudinal Study of Adolescent Health (AddHealth). The sizes of the networks we fit range from 71 to 2209 nodes. The larger networks represent more than an order of magnitude increase over the size of any network previously fit using maximum likelihood methods for models of this kind. We argue that several well-studied models in the networks literature do not fit these data well, and we demonstrate that the fit improves dramatically when the models include the recently-developed geometrically weighted edgewise shared partner (GWESP), geometrically weighted dyadic shared partner (GWDSP), and geometrically weighted degree (GWD) network statistics. We conclude that these models capture aspects of the social structure of adolescent friendship relations not represented by previous models.","tags":["degeneracy","exponential random graph model","maximum likelihood estimation","Markov chain Monte Carlo","p−star model"],"title":"Goodness of Fit of Social Network Models","type":"publication"},{"authors":["Tom A.B. Snijders","Philippa E. Pattison","Garry L. Robins","Mark S. Handcock"],"categories":[],"content":"","date":1082678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1082678400,"objectID":"be0010218e9ffbe5c12d37869ce9940d","permalink":"https:handcock.github.io/publication/cssswp_42/","publishdate":"2004-04-23T00:00:00Z","relpermalink":"/publication/cssswp_42/","section":"publication","summary":"The most promising class of statistical models for expressing structural properties of social networks is the class of Exponential Random Graph Models (ERGMs), also known as $p^*$ models. The strong point of these models is that they can represent structural tendencies, such as transitivity, that define complicated dependence patterns not easily modeled by more basic probability models. Recently, MCMC algorithms have been developed which produce approximate Maximum Likelihood estimators. Applying these models in their traditional specification to observed network data often has led to problems, however, which can be traced back to the fact that important parts of the parameter space correspond to nearly degenerate distributions, which may lead to convergence problems and a poor fit to empirical data. This paper proposes new specifications of the exponential random graph model. These specifications represent structural properties such as transitivity and heterogeneity of degrees by more complicated graph statistics than the traditional star and triangle counts. Three kinds of statistic are proposed: geometrically weighted degree distributions, alternating ktriangles, and alternating independent two-paths. Examples are presented both of modeling graphs and digraphs, in which the new specifications lead to much better results than the earlier existing specifications of the ERGM. It is concluded that the new specifications increase the range and applicability of the ERGM as a tool for the statistical analysis of social networks.","tags":["Statistical modeling","Social networks","Graphs","Transitivity","Clustering","Maximum likelihood","MCMC, p* model"],"title":"New specifications for exponential random graph models","type":"publication"},{"authors":["Susan Shortreed","Mark S. Handcock"],"categories":[],"content":"","date":1081468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1081468800,"objectID":"3b97b17c2d89dd9078c17b871cb7fc5d","permalink":"https:handcock.github.io/publication/cssswp_41/","publishdate":"2004-04-09T00:00:00Z","relpermalink":"/publication/cssswp_41/","section":"publication","summary":"Recent advances in latent space and related random effects models hold much promise for representing network data. The inherent dependency between ties in a network makes modeling data of this type difficult. In this article we consider a recently developed latent space model that is particularly appropriate for the visualization of networks. We suggest a new estimator of the latent positions and perform two network analyses, comparing four alternative estimators. We demonstrate a method of checking the validity of the positional estimates. These estimators are implemented via a package in the freeware statistical language R. The package allows researchers to efficiently fit the latent space model to data and to visualize the results.","tags":["random graph models","Markov chain Monte Carlo","visualization","maximum likelihood estimation"],"title":"Positional Estimation Within a Latent Space Model for Networks","type":"publication"},{"authors":["Mark S. Handcock"],"categories":[],"content":"","date":1072828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072828800,"objectID":"0abb0a7e0a873d0629c6659628ae67a5","permalink":"https:handcock.github.io/publication/cssswp_39/","publishdate":"2003-12-31T00:00:00Z","relpermalink":"/publication/cssswp_39/","section":"publication","summary":"This paper presents recent advances in the statistical modeling of random graphs that have an impact on the empirical study of social networks. Statistical exponential family models (Wasserman and Pattison 1996) are a generalization of the Markov random graph models introduced by Frank and Strauss (1986), which in turn are derived from developments in spatial statistics (Besag 1974). These models recognize the complex dependencies within relational data structures. A major barrier to the application of random graph models to social networks has been the lack of a sound statistical theory to evaluate model fit. This problem has at least three aspects: the specification of realistic models, the algorithmic difficulties of the inferential methods, and the assessment of the degree to which the graph structure produced by the models matches that of the data. We discuss these and related issues of the model degeneracy and inferential degeneracy for commonly used estimators.","tags":["Random graph models","log-linear network model","Markov Fields","Markov Chain Monte Carlo","Statistical Exponential Families","Psuedolikelihood"],"title":"Assessing Degeneracy in Statistical Models of Social Networks","type":"publication"},{"authors":["Mark S. Handcock","Michael S. Rendall","Jacob E. Cheadle"],"categories":[],"content":"","date":1064275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1064275200,"objectID":"d23ef1c5c0ba2c7ee370e2cf916403b0","permalink":"https:handcock.github.io/publication/cssswp_36/","publishdate":"2003-09-23T00:00:00Z","relpermalink":"/publication/cssswp_36/","section":"publication","summary":"Regression coefficients specify the partial effect of a regressor on the dependent variable. Sometimes the bivariate, or limited multivariate relationship of that regressor variable with the dependent variable is known from population-level data. We show here such population-level data can be used to reduce variance and bias about estimates of those regression coefficients from sample survey data. The method of constrained MLE is used to achieve these improvements. Its statistical properties are first described. The method constrains the weighted sum of all the covariate-specific associations (partial effects) of the regressors on the dependent variable to equal the overall population association of one or more regressors. We refer to those regressors whose bivariate or limited multivariate relationships with the dependent variable are constrained by population data as being “directly constrained.” Our study investigates the improvements in the estimation of directly-constrained variables, and also the improvements in the estimation of other regressor variables that may be correlated with the directly-constrained variables, and thus “indirectly-constrained” by the population data. The example application is to the marital fertility of black versus white women. The difference between white and black women's rates of marital fertility, available from population-level data, gives the overall association of race with fertility. The constrained MLE that uses this information both provides a far more powerful statistical test of the partial effect of being black, and purges the test of a bias that would otherwise distort the estimated magnitude of this effect. We find only trivial reductions, however, in the standard errors of the parameters for indirectly-constrained regressors.","tags":["minority group hypothesis","combining data-sets"],"title":"Generalised Linear Models Incorporating Population Level Information: An Empirical Likelihood Based Approach","type":"publication"},{"authors":["Mark S. Handcock","James Holland Jones","Martina Morris"],"categories":[],"content":"","date":1053993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1053993600,"objectID":"62c62199da9b5e081c74c74fe916ad8d","permalink":"https:handcock.github.io/publication/cssswp_31/","publishdate":"2003-05-27T00:00:00Z","relpermalink":"/publication/cssswp_31/","section":"publication","summary":"Recent work has focused attention on statistical inference for the population distribution of the number of sexual partners based on survey data. The characteristics of these distributions are of interest as components of mathematical models for the transmission dynamics of sexually-transmitted diseases (STDs). Such information can be used both to calibrate theoretical models, to make predictions for real populations, and as a tool for guiding public health policy. Our previous work on this subject has developed likelihood-based statistical methods for inference that allow for low-dimensional, semi-parametric models. Inference has been based on several proposed stochastic process models for the formation of sexual partnership networks. We have also developed model selection criteria to choose between competing models, and assessed the fit of different models to three populations: Uganda, Sweden, and the USA. Throughout this work, we have emphasized the correct assessment of the uncertainty of the estimates based on the data analyzed. We have also widened the question of interest to the limitations of inferences from such data, and the utility of degree-based epidemiological models more generally. In this paper we address further statistical issues that are important in this area, and a number of confusions that have arisen in interpreting our work. In particular, we consider the use of cumulative lifetime partner distributions, heaping and other issues raised by Liljeros et al. in a recent working paper.","tags":["social networks","epidemics","epidemiology"],"title":"On 'Sexual contacts and epidemic thresholds,' models and inference for Sexual partnership distributions","type":"publication"},{"authors":["Mark S. Handcock","James Holland Jones"],"categories":[],"content":"","date":10476e5,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":10476e5,"objectID":"660f5ff39e99c834885766feed241ef8","permalink":"https:handcock.github.io/publication/cssswp_44/","publishdate":"2003-03-14T00:00:00Z","relpermalink":"/publication/cssswp_44/","section":"publication","summary":"Epidemic thresholds in network models of heterogeneous populations characterized by highly right-skewed contact distributions can be very small. When the population is above the threshold, an epidemic is inevitable and conventional control measures to reduce the transmissibility of a pathogen will fail to eradicate it. We consider a two-sex network model for a sexually transmitted disease which assumes random mixing conditional on the degree distribution. We calculate interval estimates for the epidemic threshold for stochastic process models in three human populations based on representative surveys of sexual behavior (Uganda, Sweden, USA). For Uganda and Sweden, the epidemic threshold is greater than zero with high confidence. For the USA, the interval includes zero. We discuss the implications of these findings along with the limitations of epidemic models which assume random mixing.","tags":["social networks","epidemics","epidemiology"],"title":"Interval Estimates for Epidemic Thresholds in Two-Sex Network Models","type":"publication"},{"authors":["Mark S. Handcock","James Holland Jones"],"categories":[],"content":"","date":1043798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1043798400,"objectID":"b0239f013b6c59f9b9c66b28f901f167","permalink":"https:handcock.github.io/publication/cssswp_29/","publishdate":"2003-01-29T00:00:00Z","relpermalink":"/publication/cssswp_29/","section":"publication","summary":"Sexually-Transmitted Diseases (STDs) constitute a major public health concern. Mathematical models for the transmission dynamics of STDs indicate that heterogeneity in sexual activity level allow them to persist even when the typical behavior of the population would not support endemicity. This insight focuses attention on the distribution of sexual activity level in a population. In this paper, we develop several stochastic process models for the formation of sexual partnership networks. Using likelihood-based model selection procedures, we assess the fit of the different models to three large distributions of sexual partner counts: (1) Rakai, Uganda, (2) Sweden, and (3) the USA. Five of the six single-sex networks were fit best by the negative binomial model. The American women’s network was best fit by a power-law model, the Yule. For most networks, several competing models fit approximately equally well. These results suggest three conclusions: (1) no single unitary process clearly underlies the formation of these sexual networks, (2) behavioral heterogeneity plays an essential role in network structure, (3) substantial model uncertainty exists for sexual network degree distributions. Behavioral research focused on the mechanisms of partnership formation will play an essential role in specifying the best model for empirical degree distributions. We discuss the limitations of inferences from such data, and the utility of degree-based epidemiological models more generally.","tags":["social networks","epidemics","epidemiology"],"title":"Likelihood-Based Inference for Stochastic Models of Sexual Network Formation","type":"publication"},{"authors":["Mark S. Handcock","Eric M. Aldrich"],"categories":[],"content":"","date":1038700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1038700800,"objectID":"cf11666c9b13eb1ef6707bdc00441864","permalink":"https:handcock.github.io/publication/cssswp_27/","publishdate":"2002-12-01T00:00:00Z","relpermalink":"/publication/cssswp_27/","section":"publication","summary":"Relative distribution methods are a nonparametric statistical approach to the comparison of distributions. These methods combine the graphical tools of exploratory data analysis with statistical summaries, decomposition, and inference. This report demonstrates software for implementing relative distribution methods within the R statistical package. It describes how to download and install the software, and use it to redo the analysis in the paper 'Relative Distribution Methods' by Mark S. Handcock and Martina Morris, Sociological Methodology, Vol 28, July 1998. The full code, references and links to further resources are provided.","tags":["Empirical Likelihood","Constrained Optimisation","Generalised Linear Models"],"title":"Applying Relative Distribution Methods in R","type":"publication"},{"authors":["James Holland Jones","Mark S. Handcock"],"categories":[],"content":"","date":103464e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":103464e4,"objectID":"0dabfde9f1740bd5d4597f8a5e4f737a","permalink":"https:handcock.github.io/publication/cssswp_23/","publishdate":"2002-10-15T00:00:00Z","relpermalink":"/publication/cssswp_23/","section":"publication","summary":"Recent research into the properties of human sexual contact networks has suggested that the degree distribution of the contact graph exhibits power-law scaling. One notable property of this power-law scaling is that for a range of scaling exponents, the variance of the degree distribution is infinite. This property is of fundamental significance for the control of sexually transmitted diseases (STDs) such as HIV/AIDS since infinite variance of the degree distribution implies no epidemic threshold, and that an STD can persist regardless of its transmissibility. A stochastic process, known as preferential attachment, that yields one form of power law scaling has been suggested to underlie the scaling of sexual degree distributions. The limiting distribution of the preferential attachment process is the Yule distribution, which we fit using maximum likelihood (ML) to local network data for samples of three populations: (1) the Rakai District, Uganda, (2) Sweden, and (3) USA. For all local networks but one, our interval estimates of the scaling parameters do not overlap the range in which Yule distribution has infinite variance. The exponent for male networks in the USA is close to the infinite-variance range, but the preferential attachment model is a very poor fit to these data. We conclude that epidemic thresholds exist in both single-sex and two-sex epidemic model formulations. A strong conclusion we derive from these results is that public health interventions aimed at reducing the transmissibility of STD pathogens, such as implementing condom use or high activity anti-retroviral therapy (HAART), have the potential of bringing a population below the epidemic transition, even in populations exhibiting large degrees of behavioral heterogeneity.","tags":["social networks","epidemics","epidemiology"],"title":"An Assessment of Preferential Attachment as a Mechanism for Human Sexual Network Formation","type":"publication"},{"authors":["James Holland Jones","Mark S. Handcock"],"categories":[],"content":"","date":1020211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1020211200,"objectID":"cd1d908760cac4c9bdb0779758ed830b","permalink":"https:handcock.github.io/publication/cssswp_21/","publishdate":"2002-05-01T00:00:00Z","relpermalink":"/publication/cssswp_21/","section":"publication","summary":"There has been a growing interest in the application of social network theory to the epidemiology of sexually-transmitted diseases (STD). This interest arises from recognition that STDs are transmitted through binary contacts and, consequently, predictions about STD epidemics are unlikely to be robust to the mass-action assumptions of classical mathematical epidemiology. Substantial attention has recently been given to the possibility that human sexual networks exhibit scale-free behavior. We consider scale-free networks that arise when the probability mass function for partner number (i.e., network degree) is given by a power law, $P(k) \\approx k^{−\\alpha}$ , with $2 \u003c \\alpha \\le 3$. Such networks are characterized by a degree distribution with infinite variance, and since the epidemic threshold parameter increases linearly with the variance under behavioral heterogeneity, the epidemic threshold should always be exceeded in such a scale-free system. We derive the maximum likelihood estimator for the scaling exponent in the discrete power-law distribution, and show that the statistical fit of the power-law model is very poor for a large local network data set from Uganda. This finding suggests that human sexual networks may be more complex than simple analogy to computer networks might suggest, and that a more flexible, actor-based approach to modeling sexual networks is required to understand and control STD epidemics.","tags":["social networks","epidemics","epidemiology"],"title":"Statistical Evidence Tells Tails of Human Sexual Contacts","type":"publication"},{"authors":["Annette B. Bernhardt","Martina Morris","Mark S. Handcock","Marc A. Scott"],"categories":[],"content":"","date":993081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":993081600,"objectID":"ee04e2ba238c0873fc9a71849e9a5aab","permalink":"https:handcock.github.io/publication/book_divergent_paths/","publishdate":"2001-06-21T00:00:00Z","relpermalink":"/publication/book_divergent_paths/","section":"publication","summary":"The promise of upward mobility—the notion that everyone has the chance to get ahead—is one of this country's most cherished ideals, a hallmark of the American Dream. But in today's volatile labor market, the tradition of upward mobility for all may be a thing of the past. In a competitive world of deregulated markets and demanding shareholders, many firms that once offered the opportunity for advancement to workers have remade themselves as leaner enterprises with more flexible work forces. Divergent Paths examines the prospects for upward mobility of workers in this changed economic landscape. Based on an innovative comparison of the fortunes of two generations of young, white men over the course of their careers, Divergent Paths documents the divide between the upwardly mobile and the growing numbers of workers caught in the low-wage trap.\nThe first generation entered the labor market in the late 1960s, a time of prosperity and stability in the U.S. labor market, while the second generation started work in the early 1980s, just as the new labor market was being born amid recession, deregulation, and the weakening of organized labor. Tracking both sets of workers over time, the authors show that the new labor market is more volatile and less forgiving than the labor market of the 1960s and 1970s. Jobs are less stable, and the penalties for failing to find a steady employer are more severe for most workers. At the top of the job pyramid, the new nomads—highly credentialed, well-connected workers—regard each short-term project as a springboard to a better-paying position, while at the bottom, a growing number of retail workers, data entry clerks, and telemarketers, are consigned to a succession of low-paying, dead-end jobs.\nWhile many commentators dismiss public anxieties about job insecurity as overblown, Divergent Paths carefully documents hidden trends in today's job market which confirm many of the public's fears. Despite the celebrated job market of recent years, the authors show that the old labor market of the 1960s and 1970s propelled more workers up the earnings ladder than does today's labor market. Divergent Paths concludes with a discussion of policy strategies, such as regional partnerships linking corporate, union, government, and community resources, which may help repair the career paths that once made upward mobility a realistic ambition for all American workers.","tags":["Complex survey data","Labor Economics","Economic Inequality","data analysis","demography"],"title":"Divergent Paths: Economic Mobility in the New American Labor Market","type":"publication"},{"authors":["Mark S. Handcock","Martina Morris"],"categories":[],"content":"","date":938736e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":938736e3,"objectID":"496f0b453f4c074a98af2f0571ac00cc","permalink":"https:handcock.github.io/publication/book_reldist/","publishdate":"1999-10-01T00:00:00Z","relpermalink":"/publication/book_reldist/","section":"publication","summary":"In social science research, differences among groups or changes over time are a common focus of study. While means and variances are typically the basis for statistical methods used in this research, the underlying social theory often implies properties of distributions that are not well captured by these summary measures. Examples include the current controversies regarding growing inequality in earnings, racial diferences in test scores, socio-economic correlates of birth outcomes, and the impact of smoking on survival and health. The distributional differences that animate the debates in these fields are complex. They comprise the usual mean-shifts and changes in variance, but also more subtle comparisons of changes in the upper and lower tails of distributions. Survey and census data on such attributes contain a wealth of distributional information, but traditional methods of data analysis leave much of this information untapped. In this monograph, we present methods for full comparative distributional analysis. The methods are based on the relative distribution, a nonparametric complete summary of the information required for scale--invariant comparisons between two distributions. The relative distribution provides a general integrated framework for analysis. It offers a graphical component that simplifies exploratory data analysis and display, a statistically valid basis for the development of hypothesis-driven summary measures, and the potential for decomposition that enables one to examine complex hypotheses regarding the origins of distributional changes within and between groups. The monograph is written for data analysts and those interested in measurement, and it can serve as a textbook for a course on distributional methods. The presentation is application oriented.","tags":["Complex survey data","Census","Variance","data analysis","statistical method"],"title":"Relative Distribution Methods in the Social Sciences","type":"publication"},{"authors":["Mark S. Handcock"],"categories":[],"content":"","date":872208e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":872208e3,"objectID":"95219ebd7d68822d954545987cdb8080","permalink":"https:handcock.github.io/publication/isi97/","publishdate":"1997-08-22T00:00:00Z","relpermalink":"/publication/isi97/","section":"publication","summary":"The traditional best linear unbiased prediction procedure ('Kriging') is used in this paper for inference, but within a Bayesian framework. See Brown, Le and Zidek (1994) for an alternative Bayesian formulation. Our approach is to exam how posterior predictive distributions of areal quantities change over time. The objective is to see if there have been changes in areal temperature that are discernible from the year-to-year variation. The approach takes into account the uncertainty about the covariance function expressed in the likelihood surface and ignored by point estimates of the covariance function. These ideas are implemented for the spring temperature over the region in the northern United States based on the stations in the United States historical climatological network reported in Karl, Williams, Quinlan and Boden (1990). The results indicates that there is significant micro-scale variation over a spatially smooth field. There is substantial variation in the spring temperature from year-to-year that is spatially correlated (Figure 1). The areal mean temperature is correspondingly variable and appears to be increasing over the period considered (Figure 2). However, this finding has been confirmed using different statistical approaches (Lettenmaier, Wood and Wallis 1994). It is also evident from this later study that, had other regions or periods been picked, the results would have been quite different.","tags":["Bayesian Statistics","Climate Change","Gaussian Random Fields"],"title":"Spatial–Temporal Modeling of Meteorological Fields with application to Climate Change","type":"publication"},{"authors":["Samprit Chatterjee","Mark S. Handcock","Jeffrey S. Simonoff"],"categories":[],"content":"","date":798768e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":798768e3,"objectID":"2f6eaa2098e8dfc856e8feeb71106850","permalink":"https:handcock.github.io/publication/book_casebook/","publishdate":"1995-04-25T00:00:00Z","relpermalink":"/publication/book_casebook/","section":"publication","summary":"The most effective way to learn statistics is by actively engaging in doing the statistical analysis. This idea drives this casebook. An introductory course in statistics often fails to give the students an idea of the excitement of statistics, and its relevance in the present day world. A considerable amount of material has to be covered, with no complementary time for discussion of real life examples. Students often come away with a blurred impression of formulas, and some words like 'mean,' 'standard deviation,' and 'regression.' The point that statistical analysis is vital to arrive at conclusions in a sensible and rational manner is often neglected. This casebook is an attempt to remedy this deficiency by providing an active resource for classroom use. The book is based on cases that we have developed through almost fifty cumulative years of teaching the introductory statistics course at New York University. We have attempted in this casebook to present cases representing situations and contexts from a diverse set of fields, where statistical analysis is required to arrive at a meaningful conclusion. Topics covered include eruptions of the 'Old Faithful' geyser, the issuance of international adoption visas, the space shuttle Challenger tragedy, patterns in the Dow Jones Industrial Average and Standard and Poor's index, health expenditures of states, random drug and disease testing, baseball free agency, performance of NBA guards, energy consumption of a household, grape yields in a vineyard, and the birth and nursing of a beluga whale calf. All of the datasets are real and complete. Each case is motivated by a question that needs to be answered, and full background material is presented. The statistical analysis flows naturally from the question. The discussion given in the cases attempts to demonstrate the logic of the analysis and emphasize the interactive and iterative nature of the task. The aim of these cases is to show the reader by example that statistical analysis clarifies and throws light on a complex situation. It enables one to draw useful conclusions. Besides the final conclusion, much is learned about the problem during the analysis. The journey, as well as the arrival, matters. In addition to investigation of the specific questions raised by a particular case, we hope that the reader also will develop a feel for the kind of approach to data analysis that is likely to be fruitful in general. As statistical software has become generally available, the possibilities of superficial, but inadequate, analysis of data have increased correspondingly. However, if a data analyst is trained to develop a system of general principles in performing a data analysis that are widely applicable, it is much more likely that she will analyze future data sets in a reasonable way. It is our hope that this casebook can be helpful in highlighting the kinds of questions that need to be answered if such a system is being used.","tags":["teaching","data analysis","visualization"],"title":"A Casebook for a First Course in Statistics and Data Analysis","type":"publication"},{"authors":["F. Michelassi","W. Pomidor","A. G. Montag","J. Stephens","R. D. Goldberg","Mark S. Handcock"],"categories":[],"content":"","date":702086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":702086400,"objectID":"25650d04d3b6b80dc2766b7b1b4f9f3f","permalink":"https:handcock.github.io/publication/michelassi92/","publishdate":"1992-04-01T00:00:00Z","relpermalink":"/publication/michelassi92/","section":"publication","summary":"Semiquantitative immunohistochemical assays have been used with increasing frequency. This study was designed to investigate the reproducibility of such measurements by observing how the measured level varied du.e to (a) the choice of tissue sample from a single or multiple tumors, (b) the immunohistochemical procedure and the influence of time on staining and (c) the subjective variability between readers (interobserver) and by the same reader (intra-observer). The study was meant to judge the reproducibility of the method, not its accuracy. The choice of the monoclonal antibody therefore did not influence the results. A total of 128 sets of sljdes from 8 colonic adenocarcinomas were analyzed by three pathologists using a randomized, symmetric, prospective, doubleblind study. There was surprisingly poor agreement between readings of the same case by the three pathologists (37%) and by the same pathologist over time (58%). Based on the component of variation analysis, 11% of the total variation was due to differences in the immunohistochemical procedure, 5% to variation of expression in different tumors, 5% to interobserver and 79% to intra-observer variability. Readings of semiquantitative immunohistochemical assays is limited by subjective intrinsic variability.","tags":["biostatistics","Immunohistochemistry","assay","Adenocarcinoma"],"title":"Quantification of Variation in Reading Immunohistochemical Assays","type":"publication"},{"authors":["F. Michelassi","J. A. Hayman","Mark S. Handcock","R. Goldberg","F. Erroi","B. A. Lashner","S. B. Hanauer"],"categories":[],"content":"","date":657417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":657417600,"objectID":"3b4711dc00256bc443269aebc7e351c8","permalink":"https:handcock.github.io/publication/michelassi90/","publishdate":"1990-11-01T00:00:00Z","relpermalink":"/publication/michelassi90/","section":"publication","summary":"Increased expression of cellular oncogenes has been linked to malignant-transformation. To investigate the role of oncogenes in the malignant transformation of colonic epithelium in ulcerative colitis, we compared the levels of ras oncogene protein product (p2l) in specimens of normal human colonic mucosa (n=16) with levels in specimens ofukerative colitis with inactive (n=11) and active disease (n=11), low- (n=17) and high-grade dysplasia (n=9) and adenocarcinoma (n=13). p21 content was measured using the RAP-S monoclonal antibody in a semi-quantitative immunohistochemical assay. Titer was expressed as the highest serial dilution of antibody giving definite staining with the avidin-biotin peroxidase method. There were no statistically significant differences between ras p21-levels of low-grade and high-grade dysplasia, of active and inactive ulcerative colitis, and of ulcerative colitis with and without dysplasia. Differences in p21 titer values between normal colonic mucosa, ulcerative colitis without adenocarcinoma and adenocarcinoma in ulcerative colitis were statistically significant using Fisher's exact test (normal colonic mucosa \u003c ulcerative colitis \u003c adenocarcinoma in ulcerative colitis; all p \u003c 0.05). We conclude that p21 levels in ulcerative colitis are higher than in normal colonic mucosa and that they further increase in adenocarcinoma complicating ulcerative colitis - thus suggesting an important role for the ras oncogene in the associated malignant transformation.","tags":["biostatistics","Ras oncogene","Ulcerative colitis"],"title":"Ras Oncogene Protein Product in Ulcerative Colitis","type":"publication"},{"authors":["F. Michelassi","G. Grad","F. Erroi","M. Roncella","J. Romagnoli","Mark S. Handcock"],"categories":[],"content":"","date":654739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":654739200,"objectID":"d8ad08238ebf45437a8cc265bb3c4415","permalink":"https:handcock.github.io/publication/michelassi90hep/","publishdate":"1990-10-01T00:00:00Z","relpermalink":"/publication/michelassi90hep/","section":"publication","summary":"In order to investigate the value of ras oncogene expression as a prognostic indicator in colonic adenocarcinoma, we evaluated the level of ras gene protein product (p21) in the available material of 109 surgical specimens resected at our institution between 1978 and 1981. Pathology slides and archived paraffin blocks were retrieved for confirmation of the original diagnosis, determination of stage, and measurement of p21 content. P21 titers were obtained using the RAP-5 monoclonal antibody in a semiquantitative immunohistochemical assay. Titer was expressed as the highest dilution of antibody given definitive staining using the Avidin-Biotin peroxidase method. The analysis indicated that tumors with high ( \u003e 1:40,000) p21 titer had a lower five-year survival rate than tumors with low ( \u003c 1:40,000) titers (34.3% vs 60.8%, p \u003c 0.02). When a logistic regression analysis was used with the dependent variable being five-year survival and the independent variables being age, sex, location of tumor, Dukes' stage, mucin production, p21 titer, differentiation degree and tumor size, the statistically significant relationship of the level of ras gene protein product to long-term survival was negated by the concomitant knowledge of Dukes' stage. On the other hand, when only the variables available in the preoperative period were entered in the multivariate analysis, p21 titers retained a significant relationship with long-term survival (p \u003c 0.05). We conclude that ras oncogene determination in colonic carcinomas may have importance for the pre-operative identification of a group of colonic tumors with a more aggressive behavior and a poorer prognosis.","tags":["biostatistics","Colon","Carcinoma","Prognosis","Rasoncogene expression"],"title":"Relationship between Ras Oncogene Expression and Clinical and Pathological Features of Colonic Carcinoma","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6f2ae24689dd626422f36a03d187ca38","permalink":"https:handcock.github.io/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/featured/","section":"","summary":"Publications","tags":null,"title":"","type":"widget_page"}]